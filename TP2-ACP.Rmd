---
title: "TP2 Componentes Principales"
author: "Cristian Salinas"
date: "2023-10-07"
output: 
  html_document:
    toc: true     
    toc_depth: 2           
    toc_float: 
      collapsed: false        
      smooth_scroll: true         
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = F, message=F}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readxl)
library(GGally)
library(corrplot)
library(factoextra)
library(ggcorrplot)
library(ggrepel) #para etiquetas
```

## Ejercicio 1

```{r}
M=matrix(c(3,1,1,1,3,1,1,1,5),nrow=3,byrow=TRUE)
```

```{r}
M
```

a)  Hallar los autovalores y autovectores de la matriz de varianzas y covarian- zas.

```{r}
# Calcular los autovalores y autovectores
eigen_values <- eigen(M)

```

```{r}
# Mostrar los resultados
autovalores = eigen_values$values  # Autovalores
autovalores
```

```{r}
autovectores = eigen_values$vectors # Autovectores
autovectores
```

b)  Escribir la expresión de las componentes principales Y = (Y1,Y2, Y3)′ e indique que proporción de la variabilidad explica cada una de ellas.

$Y_1=-0.4082483 X_1 - 0.4082483 X_2 -0.8164966 X_3$

$Y_2= -0.5773503 X_1-0.5773503 X_2 +0.5773503 X_3$

$Y_3= 0.7071068 X_1 -0.7071068 X_2 - X_3-1.347436e-16$

Cada autovalor de la matriz de covarianza en un análisis de componentes principales representa la varianza (o la cantidad de variabilidad) capturada por su correspondiente componente principal. En el contexto de PCA, la suma total de todos los autovalores es igual a la varianza total de los datos originales. La proporción de la variabilidad total de los datos explicada por un componente principal específico se determina dividiendo el autovalor de ese componente principal por la suma de todos los autovalores.

En términos matemáticos, si $\lambda_1, \lambda_2, \ldots, \lambda_n$ son los autovalores de la matriz de covarianza, entonces la proporción de la variabilidad explicada por el i-ésimo componente principal se calcula como:

$$ \frac{\lambda_i}{\sum_{j=1}^{n} \lambda_j} $$

```{r}
data.frame(componentes = c('c1','c2','c3'),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(Variabilidad_acc = cumsum(Variabilidad))
```

```{r}
data.frame(componentes = c(1,2,3),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(loading_acc = cumsum(Variabilidad)) %>% 
  ggplot(aes(x= componentes, y = Variabilidad))+
  geom_point()+geom_line()+theme_bw()
```

c)  Hallar los loadings de la primer componente principal.

```{r}
autovectores[,1] #loadings de la Componente 1
```

```{r}
data.frame(loading = autovectores[,1], orden = seq(1,3,1))
```

```{r}
data.frame(loading = autovectores[,1], orden = seq(1,3,1)) %>%
  ggplot(aes(x=orden, y = loading, fill=as.factor(orden)))+
  geom_bar(stat = "identity", color='black') +
  theme_minimal() +
  labs(title = "Loadings de la CP1", x = "Orden", y = "Loading")+
  theme_bw()+ scale_fill_brewer(palette = "PuOr")+
  theme(legend.position = "none")

```

d)  Hallar los scores de las primeras dos componentes principales correspon- dientes a la observación X=(2,2,1).

```{r}
X = c(2,2,1)
X
```

```{r}
Y1= -0.4082483 * 2 -0.4082483 * 2 -0.8164966 * 1

Y2= -0.5773503 * 2 -0.5773503 * 2 + 0.5773503 * 1
```

```{r}
scores = as.vector((autovectores[,1:2] * X) %>% colSums())
scores
```

```{r}
for (i in seq(1:2) ){
  print(paste0('Y', i, '=', scores[i]))
 }
```

## Ejercicio 2 
Considerando los datos de la base chalets.xls, se pide:

```{r}
chalets <- read_excel("chalets.xls")
```

```{r}
colnames(chalets) = c('promotoraId', 'duracion', 'precio', 'superficie')
```

```{r}
chalets = chalets %>% mutate( promotoraId = as.factor(promotoraId))
chalets
```

a)  Graficar el boxplot de cada una de las variables. Indicar, si se observa, la presencia de valores atípicos.

```{r}
chalets %>% 
  pivot_longer(cols=2:4, names_to='Variable', values_to='Valor') %>% 
  ggplot(aes(y=Variable, x = Valor, fill=Variable))+
  geom_boxplot()+
  scale_fill_brewer(palette = "PuOr")+
  geom_jitter(alpha=0.5, shape=21, color='black')+
  geom_point(stat = 'summary', fun.data = mean_se, shape=12)+
  scale_color_brewer(palette = "PuOr")+
  theme_bw()+
  facet_wrap(~Variable, ncol=1, scales = "free")
```

Variables Escaladas:

```{r}
chalets %>% 
  mutate_if(is.numeric, scale) %>% 
  pivot_longer(cols=2:4, names_to='Variable', values_to='Valor') %>% 
  ggplot(aes(y=Variable, x = Valor, fill=Variable))+
  geom_boxplot()+
  scale_fill_brewer(palette = "PuOr")+
  geom_jitter(alpha=0.5, shape=21, color='black')+
  #geom_point(stat = 'summary', fun.x = 'mean')+
  scale_color_brewer(palette = "PuOr")+
  theme_bw()
```

```{r}
chalets[,2:4] %>% summary()
```

Funcion para Valores Atipicos

```{r}
select_column <- function(dataframe, column_name) {
  if (column_name %in% names(dataframe)) {
    a = dataframe[[column_name]]
   
    q3 = quantile(a, 0.75)[[1]]
    q1 = quantile(a, 0.25)[[1]]
    riq = q3-q1
    li_salvaje = (q1 - 1.5 * riq)
    ls_salvaje = (q3 + 1.5 * riq )
    li_severo = (q1 - 3 * riq)
    ls_severo = (q3 + 3 * riq )
    df = data.frame(Lim_outliers = c('Li_Salvaje','Ls_Salvaje', 'Li_Severo','Ls_Severo'), 
                    valores = c(li_salvaje, ls_salvaje, li_severo, ls_severo ))
    
    tolerance = 1e-6
    df2 = dataframe %>% mutate(Categoria = case_when(a < li_salvaje | a > ls_salvaje + tolerance ~ "Salvaje",
                                         a < li_severo | a > ls_severo + tolerance ~ "Severo",
                                         TRUE ~ "Normal")) %>% 
      filter(Categoria != 'Normal') %>% 
      #select(column_name, Categoria)
      select(all_of(column_name), Categoria)
    result = list(valores_criticos = df,
                  df_filtrado = df2)
    return(result)
  } else {
    stop("El nombre de la columna proporcionado no se encuentra en el dataframe.")
  }
}
```

```{r}
select_column(chalets, 'duracion')$valores_criticos
```

```{r}
select_column(chalets, 'duracion')$df_filtrado
```

```{r}
select_column(chalets, 'precio')$valores_criticos
```

```{r}
select_column(chalets, 'precio')$df_filtrado
```

```{r}
select_column(chalets, 'superficie')$valores_criticos
```

```{r}
select_column(chalets, 'superficie')$df_filtrado
```

b)  Graficar los diagramas de dispersión de las variables de a pares. Estimar la presencia de correlación entre variables a partir de estos gráficos, indicando si le parece fuerte y el signo de las mismas.

```{r}
chalets[,2:4] %>% ggpairs()
```

**Se aprecia correlación entre todos los pares de variables**

c)  Calcular el vector de medias y la matriz de varianzas y covarianzas muestral.

```{r}
chalets[,2:4] %>% 
  summarise_all(mean) 
```

```{r}
(chalets[,2:4] %>% 
   summarise_all(mean)) %>% 
  t() %>% 
  as.vector()
```

```{r}
cov(chalets[,2:4])
```

Tamaño del Problema = Suma de Varianzas de cada Variable.

```{r}
diag(cov(chalets[,2:4]))
sum(diag(cov(chalets[,2:4])))
```

d)  Hallar la matriz de correlación muestral. Verificar las estimaciones realizadas visualmente.

```{r}
cor(chalets[,2:4])
```

```{r}
ggcorrplot(cor(chalets[,2:4]), hc.order = TRUE, outline.col = "white", lab = 'TRUE')
```

e)   A partir de estas observaciones, le parece razonable pensar en un análisis de componentes principales para reducir la dimensión del problema?.

Coeficiente de Determinación.

```{r}
cor(chalets[,2:4])**2
```

**Si, se trata de variables con correlaciones moderadas a elevadas. Ademas, si elevamos al cuadrado los coef. de correlación obtenemos una aproximación al coeficiente de determinación R2 el cual tambien da valores elevados.**

f)  Hallar la primera componente principal y graficar sus coeficientes mediante barras verticales.

```{r}
acp = prcomp(chalets[,2:4])
acp
```

El ACP contiene 5 elementos:

1.  sdev: Este elemento contiene las desviaciones estándar de las componentes principales. La longitud de este vector es igual al número de componentes principales. En PCA, las desviaciones estándar se relacionan con la cantidad de varianza que cada componente principal captura del conjunto de datos. Al elevar al cuadrado estos valores, se obtienen los autovalores de la matriz de covarianza o correlación utilizada en el PCA.

2.  rotation: También conocido como la matriz de carga, este elemento contiene los autovectores de la matriz de covarianza/correlación de los datos. Las columnas de esta matriz son los autovectores que corresponden a las componentes principales y que se utilizan para transformar los datos originales al espacio de las componentes principales.

3.  center: Al realizar PCA, es común centrar los datos restando la media de cada variable. Este elemento contiene las medias de las variables originales que se han restado durante el proceso de centrado. Si el argumento center = FALSE se utilizó en prcomp(), este componente no estará presente.

4.  scale:Si los datos fueron escalados (normalmente dividiendo por la desviación estándar de cada variable) antes de aplicar PCA, este elemento contendrá los factores de escala utilizados. El escalado se realiza para que todas las variables contribuyan por igual al análisis, independientemente de sus unidades de medida. Si el argumento scale. = FALSE se utilizó en prcomp(), este componente no estará presente.

5.  x: Contiene los datos originales proyectados en el espacio de las componentes principales, a veces denominados como puntuaciones de las componentes principales (scores). Cada columna es una componente principal y cada fila corresponde a una observación de los datos originales.

```{r}
acp %>% str()
```

```{r}
print('sdev')
acp$sdev
print('rotation')
acp$rotation
print('center')
acp$center
print('scale')
acp$scale
print('x')
acp$x
```

Para corroborar el analisis, la suma de los valores propios deberia ser igual a la suma de las varianzas. ACP devuelve los desvios de cada Componente por lo cual es necesario elevarlo al cuadrado.

```{r}
acp$sdev**2 %>% sum()
```

```{r}
acp$rotation
```

```{r}
acp$rotation[,1] %>% 
  t() %>% 
  as.data.frame() %>% 
  pivot_longer(cols = 1:3, values_to = 'LoadingCP1', names_to = 'variable') %>% 
  ggplot(aes(x=variable, y=LoadingCP1, fill=variable))+
  geom_bar(stat='identity', color='black')+
  scale_fill_brewer(palette = "PuOr")+
  theme_bw()
  
```

g)   Indicar qué porcentaje de la variabilidad total logra explicar esta componente. Explicar si se trata de una componente de tamaño o de forma. Es posible ordenar las promotoras en función de esta componente?. Si la respuesta es afirmativa, cual es la mayor y cual la menor; si es negativa, explicar por qué no es posible ordenarlos.

```{r}
fviz_eig(acp, addlabels = T, ylim=c(0,120))
```

```{r}
acp %>% summary()

acp$rotation

acp$x
```

**La componente principal 1 (PC1) logra explicar el 97.06% de la variabilidad total de los datos. Al ser todos los loadings positivos se trata de una componente de tamañano.**

```{r}
fviz_pca_biplot(acp, ylim=c(-4,4))
```

Para ordenar las promotoras segun la primer componente seria:

```{r}
data.frame(id=seq(1:10), cp1 =acp$x[,1]) %>% 
  arrange(cp1)
```

```{r}
data.frame(id=seq(1:10), cp1 =acp$x[,1]) %>% 
  arrange(cp1) %>% ggplot(aes(x= cp1, y=0, label=id))+
  geom_point()+
  geom_text_repel(aes(label=id))+
  geom_vline(xintercept = c(-4,-8,5), linetype = 2,
             color = 2)+
  theme_bw()
```

**Es posible agrupar las promotoras según lo manifestado por la CP1. La promotoras 10 y 8 se encuentras separadas del resto. Esto quiere decir que en terminos de superficie y duracion estas promotoras se destacan con clientes de propiedades grandes e hipotecas largas. De manera contraria la promotora 1 se encuentra alejada del comportamiento promedio en terminos negativos, con propiedades chicas y duraciones cortas. Por ultimo, las promotoras 3,4,5,7 son promotoras promedio.**

Valores medios de las Variables

```{r}
acp$center
```

Para analizar el comportamiento bivariado se realizó un scaterplot entre las variables menos correlacionadas (superficie y duracion). Se puede observar que solo las promotoras 10 y 8 estan por encima de la media en ambas variables. Esto ratifica lo observado en la componente principal1.

```{r}
chalets %>% ggplot(aes(x=superficie, y=duracion))+
  geom_point()+
  geom_text_repel(aes(label=promotoraId))+
  theme_bw()+
  geom_vline(xintercept = 9.73, linetype = 2,
             color = 2)+
  geom_hline(yintercept = 19.05, linetype = 2,
             color = 2)
```

## Ejercicio 3
Dado el siguiente conjunto de datos:
$X = \begin{pmatrix} 3 & 6 \\ 5 & 6 \\ 10 & 12 \\ \end{pmatrix}$

```{r}
X = matrix(c(3, 6, 5, 6, 10, 12), nrow = 3, byrow = TRUE)
X
```
a)  Calcule la matriz de covarianza, los autovalores y autovectores.
```{r}
cov(X)
```

Tamaño Total del Problema
```{r}
diag(cov(X)) %>% sum()
```


```{r}
eigen(cov(X))
```
```{r}
autovalores = eigen(cov(X))$values
autovalores
```

Tamaño Total del Problema
```{r}
autovalores %>% sum()
```

```{r}
autovectores = eigen(cov(X))$vectors
autovectores
```

```{r}
data.frame(componentes = c('c1','c2'),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(Variabilidad_acc = cumsum(Variabilidad)*100)
```

```{r}
data.frame(componentes = c('c1','c2'),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(Variabilidad_acc = cumsum(Variabilidad)) %>% 
  ggplot(aes(x= componentes, y = Variabilidad))+ 
  geom_text_repel(aes(label=round(Variabilidad,2)*100))+
  geom_bar(stat='identity', aes(fill=componentes), color='black')+
  scale_fill_brewer(palette = 'PuOr')+
  theme_bw()
```

c)  Grafique los datos en R2 2 en la base original y en la base de los dos primeros ejes.
```{r}
data.frame(X1 = X[,1], X2 = X[,2], id=c(1,2,3)) %>% 
  ggplot(aes(x=X1, y = X2))+
  geom_point()+
  geom_text_repel(aes(label=id))+
  theme_bw()+
  ylim(c(0,12))+
  xlim(c(0,13))
```
Los Scores se pueden realizar de la siguiente manera, multiplicando las matrices.
```{r}
scores = X %*% autovectores
scores
```
Al aplicar la transformación se pueden diferenciar los puntos 1 y 2 en el sentido de las y, ya que en el original no podria hacerse ya que ambos tenian en valor 6 en Y. 
```{r}
data.frame(cp1 = scores[,1], cp2 = scores[,2], id=c(1,2,3))%>% 
  ggplot(aes(x=cp1, y = cp2))+
  geom_text_repel(aes(label=id))+
  geom_point()+
  theme_bw()
```

d)  Repita los cálculos con los datos estandarizados. Interprete los resultados
obtenidos.
```{r}
Xsc = scale(X)
Xsc
```
```{r}
cov(Xsc)
```

Tamaño Total del Problema: da diferente.
```{r}
diag(cov(Xsc)) %>% sum()
```


```{r}
eigen(cov(Xsc))
```
```{r}
autovalores_sc = eigen(cov(Xsc))$values
autovalores_sc
```

Tamaño Total del Problema
```{r}
autovalores_sc %>% sum()
```

```{r}
autovectores_sc = eigen(cov(Xsc))$vectors
autovectores_sc
```

```{r}
data.frame(componentes = c('c1','c2'),
  Variabilidad = autovalores_sc/sum(autovalores_sc)) %>% 
  mutate(Variabilidad_acc = cumsum(Variabilidad)*100)
```

Los Scores se pueden realizar de la siguiente manera, multiplicando las matrices.
```{r}
scores_sc = Xsc %*% autovectores_sc
scores_sc
```

```{r}
data.frame(cp1 = scores_sc[,1], cp2 = scores_sc[,2], id=c(1,2,3))%>% 
  ggplot(aes(x=cp1, y = cp2))+
  geom_text_repel(aes(label=id))+
  geom_point()+
  theme_bw()
```

e)  Verifique que los dos primeros autovectores son ortogonales entre sí. Represente gráficamente estos dos vectores en un gráfico bidimensional y trace rectas desde el origen hasta la ubicación de cada uno de los vectores en el gráfico.


```{r}
# Tenemos dos autovectores v1 y v2
v1 <- autovectores[,1]
v2 <- autovectores[,2]
```

```{r}
vectores_df <- data.frame(
  x = c(0, v1[1], 0, v2[1]),
  y = c(0, v1[2], 0, v2[2]),
  vector = factor(rep(c("Autovector 1", "Autovector 2"), each = 2))
)
vectores_df
```


```{r}
# Crear el gráfico con ggplot2
ggplot() +
  geom_segment(data = vectores_df, aes(x = 0, y = 0, xend = x, yend = y, colour = vector), 
               arrow = arrow(type = "closed", length = unit(0.2, "inches"))) +
  scale_color_manual(values = c("Autovector 1" = "blue", "Autovector 2" = "red")) +
  theme_minimal() +
  labs(title = "Autovectores en el Espacio Bidimensional Sin Escalar", x = "X", y = "Y") +
  coord_fixed()
```

```{r}
v1 <- autovectores_sc[,1]
v2 <- autovectores_sc[,2]
vectores_df <- data.frame(
  x = c(0, v1[1], 0, v2[1]),
  y = c(0, v1[2], 0, v2[2]),
  vector = factor(rep(c("Autovector 1", "Autovector 2"), each = 2))
)
vectores_df
```

```{r}
# Crear el gráfico con ggplot2
ggplot() +
  geom_segment(data = vectores_df, aes(x = 0, y = 0, xend = x, yend = y, colour = vector), 
               arrow = arrow(type = "closed", length = unit(0.2, "inches"))) +
  scale_color_manual(values = c("Autovector 1" = "blue", "Autovector 2" = "red")) +
  theme_minimal() +
  labs(title = "Autovectores en el Espacio Bidimensional Datos Escalados", x = "X", y = "Y") +
  coord_fixed()
```

La razón por la que los autovectores y autovalores resultan diferentes cuando se calculan con datos escalados y sin escalar radica en cómo la escala de los datos afecta a la matriz de covarianza o correlación que se utiliza para obtenerlos.

*Datos sin escalar*
Cuando realizas un análisis de componentes principales (PCA) sin escalar los datos, estás trabajando con la varianza y covarianza "naturales" de tus datos. Si las variables tienen diferentes unidades o rangos de varianza, las variables con mayor varianza tendrán un peso más significativo en el resultado del PCA. Esto significa que las componentes principales resultantes pueden estar sesgadas hacia estas variables.

*Datos escalados*
Al escalar los datos, normalmente se centran (restando la media) y se estandarizan (dividiendo por la desviación estándar). Este proceso lleva a todas las variables a una misma escala y elimina las unidades, lo que permite que cada variable contribuya equitativamente al análisis del PCA, independientemente de su varianza original o unidades de medida. En otras palabras, el escalado asegura que la varianza de una variable no influya más que otra simplemente debido a la magnitud de sus valores.

*Efecto en Autovalores y Autovectores*
Los autovalores reflejan la varianza capturada por cada componente principal. Si las variables no están escaladas, las componentes principales reflejarán más la varianza de las variables con rangos más grandes. Cuando las variables están escaladas, los autovalores reflejarán una contribución más equitativa.

Los autovectores, que indican la dirección de las componentes principales, también cambian. En datos no escalados, las direcciones pueden estar dominadas por variables con mayor varianza. En datos escalados, cada variable tiene igual oportunidad de influir en la dirección de las componentes principales.

*Conclusión*
La elección de escalar o no los datos antes de realizar el PCA depende del contexto y de los objetivos del análisis. Si las variables están en diferentes unidades o tienen diferentes rangos de varianza y deseas que todas tengan la misma importancia en el análisis, debes escalar. Si algunas variables son intrínsecamente más variables y eso es importante para el análisis, podrías optar por no escalar.

## Ejercicio 4
Sea S la matriz de varianzas y covarianzas poblacionales:
$X = \begin{pmatrix} 3 & 1 & 1 \\ 1 & 4 & 0 \\ 1 & 0 & 2 \\ \end{pmatrix}$
```{r}
S = matrix(c(3, 1, 1, 1, 4, 0, 1, 0, 2), nrow = 3, byrow = TRUE)
S
```

* X1 puntuación media obtenida en las asignaturas de econometría
* X2 puntuación media obtenida en las asignaturas de derecho
* X3 puntuación media obtenida en asignaturas libres

Crear una matriz de correlacion a partir de una matriz de covarianzas
```{r}
# Calcular las desviaciones estándar de las variables originales
std_devs <- sqrt(diag(S))
# Crear una matriz de desviaciones estándar para la división
std_dev_matrix <- outer(std_devs, std_devs, "*")
# Calcular la matriz de correlación
cor_matrix <- S / std_dev_matrix
# Mostrar la matriz de correlación
print(cor_matrix)
```
Sí, es posible convertir una matriz de covarianza en una matriz de correlación utilizando operaciones matemáticas estándar y la matriz identidad, junto con la matriz de desviaciones estándar de las variables. La idea es normalizar la matriz de covarianza de tal manera que las varianzas (los elementos de la diagonal) se conviertan en 1, lo cual es característico de una matriz de correlación.

En el contexto de matrices, la diagonal de la matriz de covarianza contiene las varianzas de las variables, y para convertirla en una matriz de correlación, debemos dividir cada elemento de la matriz de covarianza por el producto de las desviaciones estándar asociadas a sus respectivas filas y columnas.

Si D es una matriz diagonal donde cada elemento diagonal D[i, i] es la desviación estándar de la variable i, entonces la matriz de correlación R se puede obtener de la siguiente manera:

$R=D^{−1}⋅C⋅D^{−1}$
donde C es la matriz de covarianza y $D^{-1}$ es la inversa de la matriz D (que es simplemente 1 dividido por cada elemento en la diagonal de D, ya que D es una matriz diagonal).

```{r}
# La matriz diagonal D con las desviaciones estándar
D <- diag(sqrt(diag(S)))

# Invertir D para obtener D^-1
D_inv <- solve(D)

# Calcular la matriz de correlación
cor_matrix <- D_inv %*% S %*% D_inv

# Mostrar la matriz de correlación
print(cor_matrix)

```

a) Calcule los autovalores de la matriz S.
```{r}
autovalores = eigen(S)$values
autovectores = eigen(S)$vectors

autovalores
autovectores
```

```{r}
autovalores %>% sum()
diag(S) %>% sum()
```

b) Interprete la segunda componente principal sabiendo que el autovector correspondiente:
$e1 =(0,5744;−0,5744;0,5744).$

```{r}
autovectores[,2]
```
Varianza explicada:
```{r}
autovalores/sum(autovalores)*100

autovalores[2]/sum(autovalores)*100
```

Para el analisis es necesario recordar de que se tratan las variables. 

* X1 puntuación media obtenida en las asignaturas de econometría
* X2 puntuación media obtenida en las asignaturas de derecho
* X3 puntuación media obtenida en asignaturas libres


*X1* (Econometría): El peso positivo (0.5744) de la puntuación media obtenida en las asignaturas de econometría sugiere que hay una asociación directa entre las puntuaciones altas en econometría y los valores altos en la segunda componente principal. Es decir, a medida que las puntuaciones en econometría aumentan, también lo hace el valor de la segunda componente principal.

*X2* (Derecho): El peso negativo (-0.5744) indica que las puntuaciones altas en derecho están asociadas inversamente con la segunda componente principal. Un valor más alto en derecho contribuiría a un valor más bajo en esta componente principal. Esto podría sugerir que los estudiantes que obtienen altas puntuaciones en derecho tienden a obtener puntuaciones más bajas en econometría y asignaturas libres, o viceversa.

*X3* (Asignaturas Libres): Al igual que con X1, el peso positivo (0.5744) significa que las puntuaciones más altas en asignaturas libres se asocian con valores más altos en la segunda componente principal.

La igualdad de los valores absolutos de los pesos indica que la contribución de cada asignatura a la variabilidad capturada por esta componente principal es igual en magnitud. Además, el hecho de que los pesos de X1 y X3 sean positivos y el de X2 sea negativo sugiere que esta componente podría estar capturando un contraste en el desempeño académico: estudiantes que se desempeñan de manera similar en econometría y asignaturas libres pero de manera opuesta en derecho.

En resumen, la segunda componente principal podría estar identificando un patrón de desempeño estudiantil donde existe una dicotomía entre el rendimiento en econometría y asignaturas libres frente al rendimiento en derecho. Estudiantes con altos valores en esta componente principal tenderían a tener puntuaciones similares en econometría y asignaturas libres, pero bajas en derecho, y aquellos con bajos valores en la componente principal mostrarían el patrón opuesto.

*Dirección y Ponderación*: Las tres variables originales contribuyen de manera igual en magnitud a esta componente principal, ya que los valores absolutos de las entradas del autovector son iguales.

*Signo*: El signo de las entradas del autovector indica la dirección de la relación. En este caso, la segunda entrada es negativa, mientras que la primera y la tercera son positivas. Esto significa que la segunda variable está inversamente relacionada con la primera y la tercera en esta componente principal.

*Interpretación de la Componente*: La segunda componente principal parece capturar una variabilidad en los datos donde la primera y la tercera variables varían juntas en la dirección opuesta a la segunda variable. Cabe destacar que la primer variable esta correlacionada positivamente con las variables 2 y 3. Mientras que la segunda variable tiene 0 correlacion con la tercera.

*Varianza Explicada*: La componente explica el 33% de la variacion de los datos y la igualdad de los valores en el autovector sugiere que esta componente está equidistantemente orientada con respecto a las tres variables.

*Relevancia de la Componente*: Asumiendo que esta es la segunda componente principal en una PCA, podría representar la segunda mayor fuente de variabilidad en el conjunto de datos después de la primera componente principal.

En resumen, la segunda componente principal parece representar una dimensión de los datos donde hay un equilibrio de variabilidad entre las tres variables observadas, con la segunda variable moviéndose en la dirección opuesta a las otras dos. Para una interpretación más detallada y significativa, necesitaríamos entender qué representan estas variables en el contexto de los datos analizados.

c) Como se debería interpretar si un estudiante tuviera una puntuación en la componente principal muy inferior a la de sus compañeros?.

*Econometría (X1)*: Dado que la carga asociada con la econometría es positiva, una puntuación baja en la segunda componente principal sugiere que el estudiante probablemente tiene una puntuación baja en econometría, o al menos más baja en comparación con la puntuación promedio de sus compañeros.

*Derecho (X2):* La carga negativa asociada con las asignaturas de derecho implica que una puntuación baja en la segunda componente principal indica una puntuación alta en derecho. En otras palabras, el estudiante puede estar destacando en derecho en comparación con sus compañeros.

*Asignaturas Libres (X3):* Al igual que con la econometría, una carga positiva para las asignaturas libres significa que una puntuación baja en esta componente principal sugiere que el estudiante también tiene un rendimiento bajo en asignaturas libres.

Dado el patrón de cargas en el autovector de la segunda componente principal, una puntuación baja para un estudiante indicaría *un rendimiento académico que contrasta con las asignaturas de economía y libres en comparación con el derecho.* Es decir, este estudiante podría tener un *perfil académico fuerte en derecho pero débil en econometría y asignaturas libre*s, lo que refleja una inclinación hacia el área legal sobre las otras áreas.

Esto destaca la importancia del contexto en la interpretación de las componentes principales. La segunda componente principal parece capturar una dimensión de variabilidad en el rendimiento académico que distingue entre el enfoque en el derecho y otras áreas de estudio. 
*Por lo tanto, una puntuación baja en esta componente no es necesariamente indicativa de un rendimiento académico general pobre, sino más bien de una distribución específica de fortalezas y debilidades académicas.*

d) ¿Cuántas componentes principales serán necesarias para explicar al menos el 80 % de la variabilidad total del conjunto?
```{r}
cumsum(autovalores/sum(autovalores)*100)
```

```{r}
data.frame(componentes = c(1,2,3),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(loading_acc = cumsum(Variabilidad)) 
```
*Para obtener al menos el 80% harian falta las componentes 1 y 2.*

```{r}
data.frame(componentes = c(1,2,3),
  Variabilidad = autovalores/sum(autovalores)) %>% 
  mutate(loading_acc = cumsum(Variabilidad)) %>% 
  ggplot(aes(x= componentes, y = Variabilidad))+
  geom_point()+geom_line()+theme_bw()
```

# Ejercicio 6
La tabla gorriones.xls contiene datos de 49 aves, 21 de los cuales sobrevi- vieron a una tormenta. Se pide:

```{r}
gorriones <- read_excel("gorriones.xlsx")
```

```{r}
gorriones = gorriones %>% mutate(sobrevida = case_when(sobrevida == 1 ~ "Si",
                                           sobrevida == -1 ~"No"),
                                 sobrevida = as.factor(sobrevida))
gorriones
```

a) Estandarice las variables y calcule la matriz de covarianzas para las variables estandarizadas.

```{r}
gorriones_sc= gorriones %>% select(-c(pajaro, sobrevida)) %>% 
  mutate_if(is.numeric, ~scale(.))
gorriones_sc
```
Boxplots Estandarizados
```{r}
gorriones[,2:6] %>% 
  mutate_if(is.numeric, scale) %>% 
  pivot_longer(cols=1:5, names_to='Variable', values_to='Valor') %>% 
  ggplot(aes(x=Variable, y = Valor, fill=Variable))+
  geom_boxplot()+
  scale_fill_brewer(palette = "PuOr")+
  geom_jitter(alpha=0.5, shape=21, color='black')+
  #geom_point(stat = 'summary', fun.x = 'mean')+
  scale_color_brewer(palette = "PuOr")+
  theme_bw()
```

Boxplots Sin Estandarizar
```{r}
gorriones %>% 
  pivot_longer(cols=2:6, names_to='Variable', values_to='Valor') %>% 
  ggplot(aes(x=Variable, y = Valor, fill=Variable))+
  geom_boxplot()+
  scale_fill_brewer(palette = "PuOr")+
  geom_jitter(alpha=0.5, shape=21, color='black')+
  geom_point(stat = 'summary', fun.data = mean_se, shape=12)+
  scale_color_brewer(palette = "PuOr")+
  theme_bw()+
  facet_wrap(~Variable, ncol=5, scales = "free")
```
```{r}
cov(gorriones_sc) %>% round(3)
```
b) Verifique que ésta es la matriz de correlación de las variables originales.
**La matriz de correlacion de los datos sin estandarizas es igual a la matriz de varianzas de los datos estandarizados**.
```{r}
cor(gorriones[,2:6]) %>% round(3)

cov(gorriones_sc) %>% round(3) == cor(gorriones[,2:6]) %>% round(3)

all.equal(cov(gorriones_sc) %>% round(3) , cor(gorriones[,2:6]) %>% round(3))
```
c) Le parece adecuado en este caso un análisis de componentes principales. ¿Qué indica el autovalor para una componente principal?
**Si, todas las variables tienen correlación positiva**

En el contexto del Análisis de Componentes Principales (PCA), un autovalor asociado a una componente principal **indica la cantidad de varianza que dicha componente principal captura de los datos originales.** En términos más técnicos, un autovalor refleja la proporción de la varianza total que es explicada por su componente principal correspondiente.

**Magnitud de Varianza:** Un autovalor grande indica que esa componente principal captura una gran parte de la varianza en los datos. Por el contrario, un autovalor pequeño sugiere que la componente principal correspondiente capta solo una pequeña cantidad de la varianza.

**Orden de Importancia:** Las componentes principales se ordenan típicamente por la magnitud de sus autovalores, de mayor a menor. La primera componente principal es la dirección en el espacio de características en la que los datos varían más, seguida de la segunda componente principal, y así sucesivamente.

**Elección de Componentes:** En muchas aplicaciones de PCA, solo las componentes principales con los autovalores más altos se retienen para reducir la dimensionalidad de los datos. Las componentes con autovalores bajos a menudo se descartan porque capturan menos información (varianza).

**Total de Varianza Explicada:** La suma de todos los autovalores de un PCA es igual a la suma de las varianzas de todas las variables originales. Esto se debe a que PCA es una transformación ortogonal que conserva la varianza total de los datos.

**Contribución Porcentual:** La contribución porcentual de cada componente principal a la varianza total se calcula dividiendo el autovalor de la componente por la suma total de los autovalores.

**Decisión sobre la Reducción de Dimensiones:** A menudo se utiliza un "criterio de corte" como el gráfico de sedimentación (scree plot) o el criterio de Kaiser (mantener solo componentes con autovalores mayores que 1) para decidir cuántas componentes principales retener. También se puede utilizar el porcentaje acumulado de varianza explicada como criterio.

```{r, warning=F, message=F}
gorriones[,2:6] %>% ggpairs()
```

d) ¿Cuántas componentes son necesarias para explicar el 80 % de la varianza total? Realice el grafico de sedimentación, fundamente su respuesta con este gráfico.

```{r}
acp_gorriones = prcomp(gorriones_sc)
acp_gorriones
```
```{r}
summary(acp_gorriones)
```
**Hacen falta las dos primeras componentes para representar al menos el 80% de la variabilidad**
```{r}
fviz_eig(acp_gorriones, addlabels = T, ylim=c(0,100))
```

e) ¿Cuál es la expresión de la primer componente principal?

Totas las CP.
```{r}
cp_gorriones = acp_gorriones$rotation
cp_gorriones
```
La primer componente:
```{r}
cp_gorriones[,1]
```

f) ¿Cómo queda expresada la primer componente principal? (en función del autovector correspondiente y de las variables).

$CP1 = largototal * 0.4549833 + extension * 0.4602276 +  cabeza * 0.4478919 + humero * 0.4727851 + esternon * 0.3962927$

g) Encuentre las coordenadas del pájaro 11 en las nuevas componentes.

```{r}
gorriones %>% filter(pajaro==11)
```
```{r}
p11 = gorriones %>% 
  filter(pajaro==11) %>% 
  select(-c(pajaro, sobrevida))%>% 
  t() %>%
  as.vector()
p11
```

CP1:
```{r}
cp_gorriones[,1]
```


```{r}
cp_gorriones[,1] %*% p11
```

```{r}
p11[1] * 0.4549833 + p11[2] * 0.4602276 +  p11[3] * 0.4478919 + p11[4] * 0.4727851 + p11[5] * 0.3962927
```

h) Represente gráficamente en el plano. (Eje 1 vs 2, 1 vs 3, 2 vs 3). Interprete los tres primeros ejes.
```{r}
fviz_pca_biplot(acp_gorriones, geom = c("point", "text"), 
                col.ind = "cos2", # Color por calidad de la representación
                col.var = "contrib", # Color por contribución a la PCA
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = TRUE # Evitar solapamiento de texto
                )
```
```{r}
fviz_pca_biplot(acp_gorriones, axes = c(1, 3), geom = c("point", "text"), 
                col.ind = "cos2", # Color por calidad de la representación
                col.var = "contrib", # Color por contribución a la PCA
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = TRUE # Evitar solapamiento de texto
                )
```
```{r}
fviz_pca_biplot(acp_gorriones, axes = c(2, 3), geom = c("point", "text"), 
                col.ind = "cos2", # Color por calidad de la representación
                col.var = "contrib", # Color por contribución a la PCA
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = TRUE # Evitar solapamiento de texto
                )
```

i) Realice un gráfico donde se observen los gorriones en los nuevos ejes 1 y 2, y resalte con distinto color el grupo de los que sobrevivieron.
```{r}
cbind(acp_gorriones$x, gorriones$sobrevida) %>% 
  data.frame() %>% 
  select(PC1, PC2, V6) %>% 
  mutate(sobrevida = case_when(V6 == 2 ~ "Si", V6 == 1 ~"No") ) %>% 
  ggplot(aes(x=PC1, y=PC2, color=sobrevida))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +  # Línea horizontal en y = 0
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +  # Línea vertical en x = 0
  theme_bw()
```

j) Utilice el Análisis en Componentes Principales como método para encontrar outliers.



